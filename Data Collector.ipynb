{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the imports from the sklearn module are statistical tools to help perform K-Means Clustering\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.cluster import KMeans\n",
    "# imports pandas and numpy, two important libraries for data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# imports cv2, a computer vision library to help manipulate images\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write a utlity function to determine if a number is in a range. This makes the code less cumbersome later\n",
    "def is_between (lower, num, upper):\n",
    "    return lower <= num <= upper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# While averages are used as the default, some utility functions were returned to help with other, more advanced\n",
    "# summary statistics which may help in some situations the above_nth() and below_nth() functions in particular return\n",
    "# an array above or below a percentile. This may be helpful when trying to isolate a relatively dark or light part of \n",
    "# a flower or other feature of interest\n",
    "# These functions are strictly optional, but may be useful\n",
    "\n",
    "# A utlity function that returns an array above a pre-defined percentile\n",
    "# ie, returns all values above the nth percentile, inclusive\n",
    "def above_nth (np_array, n):\n",
    "    \n",
    "    result = np_array >= np.percentile(np_array, n)\n",
    "    \n",
    "    return np_array[result]\n",
    "\n",
    "# A utlity function that returns an array above a pre-defined percentile\n",
    "# ie, returns all values below the nth percentile, inclusive\n",
    "def below_nth(np_array, n):\n",
    "    \n",
    "    result = np_array <= np.percentile(np_array, n)\n",
    "    \n",
    "    return np_array[result]\n",
    "\n",
    "# Write a function to return the percent of returned pixels\n",
    "# The first argument is meant to be an array of pixels within a cluster\n",
    "# The second and third arguments are the width and height of the image\n",
    "# The area of the cluster is then returned as a percent of the entire area\n",
    "def percent_pix (np_array, image_dim_1, image_dim_2):\n",
    "    \n",
    "    total_pixels = image_dim_1 * image_dim_2\n",
    "    \n",
    "    return len(np_array)/total_pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Receives an image path and number of clusters k and returns a dictionary of clusters and values\n",
    "# This function works very similarly to the function of the same name in the Image Summary Visualizer,\n",
    "# but there's a key difference in how it aggregates clusters within a defined color range\n",
    "# The image_source argument is a path to the image\n",
    "# k is the k for K-Means Clustering\n",
    "# lower_bound is the lower bound of the HSV values\n",
    "# upper_bound is the upper bound of the HSV values\n",
    "def image_summary(image_source, k, lower_bound, upper_bound):\n",
    "    \n",
    "    # Load image\n",
    "    image = cv2.imread(image_source)\n",
    "    \n",
    "    try:\n",
    "        # Very large images take a lot of time to process, so if it has a width of over 1500, the image is resized to \n",
    "        # @ 1/4th of its original size\n",
    "        if image.shape[0] > 1500:\n",
    "            image = cv2.resize(image, (int(image.shape[0]/4), int(image.shape[1]/4)))\n",
    "        \n",
    "        # Convert image to HSV\n",
    "        image_hsv = cv2.cvtColor(image,cv2.COLOR_BGR2HSV)\n",
    "        \n",
    "        # By default, images are stored as 3D objects with a width and height defined by the image resolution\n",
    "        # In addition, each pixel stores three pieces of information pertaining to color, in this case, H, S, and V\n",
    "        # Mathematrical operations on this 3D object is cumbersome and inefficient, so the flatten() method\n",
    "        # turns it into a 2D array for quicker operations.\n",
    "        # The first three values in the 2D array correspond with the 3 color values in the top-left pixel\n",
    "        # The next three values correspond with the 3 color values in next pixel to the right, etc.\n",
    "        # For example, the data structure of an image typically look like this:\n",
    "        # [[[1,2,3], [4,5,6]],\n",
    "        # [[7,8,9], [10,11,12]]\n",
    "        # The flatten argument transforms it to this:\n",
    "        # [1,2,3,4,5,6,7,8,9,10,11,12]\n",
    "        # Flatten the image for more efficient processing\n",
    "        image_flat = image_hsv.flatten()\n",
    "        \n",
    "        # Because of how the image is flatten, we know that the H, S, and V values reoccur at every third pixel\n",
    "        # at a slight different index i.e. All H values are at the 0th place, 3rd place, 6th place, 9th place and so on\n",
    "        # We take advantage of this to collect all the H, S, and V values into single variables\n",
    "\n",
    "        # the h-channel. Takes every 3rd value from flatten image starting at 0th place\n",
    "        h = image_flat[0::3]\n",
    "        # the s-channel. Takes every 3rd value from flatten image starting at 1st place\n",
    "        s = image_flat[1::3]\n",
    "        # the v-channel. Takes every 3rd value from flatten image starting at 2nd place\n",
    "        v = image_flat[2::3]\n",
    "\n",
    "        # Create a dataframe from the pixel values\n",
    "        df = pd.DataFrame(data = h, columns = [\"h\"])\n",
    "        df['s'] = s\n",
    "        df['v'] = v\n",
    "        \n",
    "        # H, S, and V values are at different scales. H values vary from 0-179 and S and V vary from 0-255\n",
    "        # Consequently, any clustering based on a geometric distance will be skewed\n",
    "        # To remedy this, the StandScaler() function brings everything onto a standard scale\n",
    "\n",
    "        # initialize scaler\n",
    "        scaler = StandardScaler()\n",
    "\n",
    "        # Scaled features\n",
    "        scaler.fit(df)\n",
    "        df_scaled = scaler.transform(df)\n",
    "\n",
    "        # These two lines actually perform the K-Means clustering. Note that k is the same k passed in the initial argument\n",
    "        kmeans = KMeans(n_clusters = k)\n",
    "        kmeans.fit(df_scaled)\n",
    "\n",
    "        # Create a new column in the dataframe for the labels\n",
    "        # The labels themselves are simple numeric values that denote which pixels go into which cluster\n",
    "        # (ie, all pixels labeled as 1 are in the same cluster)\n",
    "        df[\"cluster\"] = kmeans.labels_\n",
    "\n",
    "        # create an empty dictionary to store summary values\n",
    "        colors = {}\n",
    "\n",
    "        # Iterates through the clusters, calculates the summary statistics (if they're in the upper and lower bounds)\n",
    "        for i in range(k):\n",
    "\n",
    "            # filter the dataframe by cluster\n",
    "            df_filter = df[df[\"cluster\"] == i]\n",
    "\n",
    "            # Find the mean of h\n",
    "            h_mean = np.mean(df_filter[\"h\"])\n",
    "\n",
    "            # Find out if h_mean is within range. If not, the loop moves to the next iteration\n",
    "            # If it does it moves to the next item to calculate. This method should speed up the code\n",
    "            # because it doesn't need to calculate everything every time. The drawback is that more \n",
    "            # than a handful of calculations make the code cumbersome/hard to read\n",
    "            if is_between(lower_bound[0], h_mean, upper_bound[0]):\n",
    "\n",
    "                # Find the mean of s\n",
    "                s_mean = np.mean(df_filter[\"s\"])\n",
    "\n",
    "                # Next comparison\n",
    "                if is_between(lower_bound[1], s_mean, upper_bound[1]):\n",
    "\n",
    "                    # Find the mean of v\n",
    "                    v_mean = np.mean(df_filter[\"v\"])\n",
    "\n",
    "                    # Next comparison\n",
    "                    if is_between(lower_bound[2], v_mean, upper_bound[2]):\n",
    "\n",
    "                        # store ONLY the averages that fit in range in the dictionary\n",
    "                        # If the values are calculated need to be changed, this is point to do it\n",
    "                        colors[i] = [h_mean, s_mean, v_mean]\n",
    "\n",
    "        # Once the summary stats for the clusters are calculated, some additional checks need to be made\n",
    "        \n",
    "        # If there's no clusters in range, then return \"no flowers\"\n",
    "        if len(colors) == 0:\n",
    "            return \"no flowers\"\n",
    "        # If there's only 1 cluster in range, just return the values as is\n",
    "        elif len(colors) == 1:\n",
    "            return list(colors.values())[0]\n",
    "        # If there are multiple clusters in range, they need to be combined and their summary stats re-calculated\n",
    "        else:\n",
    "\n",
    "            # Create a list of dataframes to combine\n",
    "            df_combine = [df[df[\"cluster\"] == key] for key in colors.keys()]\n",
    "\n",
    "            # Concatenate the dataframes into one, bigger, dataframe\n",
    "            df_result = pd.concat(df_combine)\n",
    "\n",
    "            # return a list of values\n",
    "            # if you want to change which measure to evaluate, here's the place to choose. Using Numpy's built-in\n",
    "            # functions are recommended as they're usually quicker\n",
    "            result =  [\n",
    "                np.mean(df_result[\"h\"]), \n",
    "                np.mean(df_result[\"s\"]),\n",
    "                np.mean(df_result[\"v\"])\n",
    "            ]\n",
    "            \n",
    "            return result\n",
    "\n",
    "    except:\n",
    "        return \"An error occured\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define upper and lower bounds\n",
    "lower_bound = (123, 15, 0)\n",
    "upper_bound = (157, 255, 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data from a CSV file and stores it into a dataframe\n",
    "# Like the destination, an \"r\" should be added in front path\n",
    "# This may be either an absolute path (recommended for beginners) or a relative path, depending on how the file is saved\n",
    "# Note: most of the time, no further arguments are required. Sometimes, though, there will be encoding issues\n",
    "# The most common fix is to add an argument, \"encoding\", using either:\n",
    "# encoding = 'utf-8'\n",
    "# encoding = 'latin1', \n",
    "# encoding = 'iso-8859-1',\n",
    "# encoding = 'cp1252'\n",
    "df = pd.read_csv(r\"C:\\Users\\Example\\FlowerClassification\\data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All that code above supports these final lines\n",
    "# This creates a new column in the dataframe called \"KMeansData\"\n",
    "# This assumes there is already a column in the dataframe called \"path\" which has the path to the image\n",
    "# In the image_summary() function, the first argument is always x, because in this case, x will take \n",
    "# the value of the path to the image\n",
    "# The second argument is k for K-Means clustering\n",
    "# The third and fourth arguments are the lower and upper bound for HSV values, already defined in other variables\n",
    "# NOTE: Depending on the size of the data set and the number k selected, this may take some time to complete\n",
    "# In smaller datasets with a low k, this may take 30 minutes to an hour\n",
    "# For larger datasets with larger k, this may run overnight or longer\n",
    "df[\"KMeansData\"] = df[\"path\"].apply(lambda x: image_summary(x, 5, lower_bound, upper_bound))\n",
    "\n",
    "# Finally, the code is saved to a csv file locally\n",
    "# Use just a file name to save it in the same directoty\n",
    "# Use an absolute path to save it to some other location in the system\n",
    "df.to_csv(\"YourSaveFile.csv\")\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
